# LLM Provider Configuration
# Choose your preferred LLM provider for natural language to SQL translation
LLM_PROVIDER=anthropic
LLM_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Alternative LLM Providers:
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o
# OPENAI_API_KEY=your_openai_api_key_here

# LLM_PROVIDER=cohere
# LLM_MODEL=command-r-plus
# COHERE_API_KEY=your_cohere_api_key_here

# Database Configuration
# Choose ONE of the following database configurations:

# SQLite (default)
DATABASE_TYPE=sqlite3
DATABASE_PATH=./nttp.db

# PostgreSQL
# DATABASE_TYPE=pg
# DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# MySQL
# DATABASE_TYPE=mysql2
# DATABASE_URL=mysql://user:password@localhost:3306/dbname

# SQL Server
# DATABASE_TYPE=mssql
# DATABASE_URL=Server=localhost,1433;Database=dbname;User Id=user;Password=password;Encrypt=true

# Server Configuration
LOG_LEVEL=INFO
MAX_QUERY_LENGTH=500
DEFAULT_LIMIT=100
MAX_LIMIT=1000

# 3-Layer Cache Configuration
# L1: Exact match cache (hash-based, <1ms, $0)
# L2: Semantic cache (embedding-based, ~80ms, ~$0.0001)
# L3: LLM fallback (existing pipeline, ~2s, ~$0.01)

# Embedding Provider (for L2 semantic cache)
# Supported: openai, cohere, mistral, google
# EMBEDDING_MODEL is optional - defaults to provider-specific model:
#   - openai: text-embedding-3-small
#   - cohere: embed-v4.0
#   - mistral: mistral-embed
#   - google: text-embedding-004
EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small  # Optional, uses default if not set
OPENAI_API_KEY=your_openai_api_key_here

# Alternative: Use Cohere for embeddings
# EMBEDDING_PROVIDER=cohere
# EMBEDDING_MODEL=embed-v4.0  # Optional, auto-detected for Cohere
# COHERE_API_KEY=your_cohere_api_key_here

# Cache Size Limits
L1_CACHE_SIZE=1000
L2_CACHE_SIZE=500

# Semantic similarity threshold (0-1)
# Recommended: 0.80-0.85 for natural language queries
SIMILARITY_THRESHOLD=0.85
